%% report.tex
%%
%% TP MLEA - d-hall_f

\documentclass[pdftex]{article}

\usepackage[dvips]{graphicx,color}
\usepackage[pdftex,colorlinks]{hyperref}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}

\sloppy

\begin{document}

\title{MLEA - Rapport de TP 2}
\author{Florent D'Halluin\and Hervé Cuche}
\date{14 Novembre 2009}

\maketitle

\section*{Quickstart}

\bigskip

Parcours de correction rapide et efficace:

\begin{itemize}
\item{\verb-make- dans un term libre (Compilation des tests et
  géneration des images en arrière-plan --- peut prendre 2 heures).}
\item{Lecture du rapport.}
\item{\emph{Optionnel: Ctrl-c le make (les images les plus
    significatives sont générées en premier dans}
    \verb-classification/images-\emph{).}}
\item{Jeter un oeil aux images générées, tester quelques commandes.}
\item{Noter.}
\end{itemize}

\bigskip

\emph{Note: L'architecture des fichiers rendus a été reprise des TPs de
MLEA1 (Florent D'Halluin).  Elle comprend tout le travail effectué
jusqu'ici.}

\section{ID3}

\verb+cd classification+.
\verb-make- (génère les images pour toutes les sous-sections).

\subsection{Notes sur l'implémentation}

\bigskip

Test ID3:
\begin{verbatim}
make <Dataset>
python id3.py <Dataset>_data [<Discretization>] [<Gain>] [--prune] --test
dot -Tpdf <Dataset>_data_graph.dot -o <Dataset>_data_id3_graph.pdf
display <Dataset>_data_id3_graph.pdf
\end{verbatim}

\bigskip

Classification:
\begin{verbatim}
make <Dataset>
python classification.py <Dataset>_data <Positive label> <Data Size>
\end{verbatim}
\emph{Note: \'Etant donné le grand nombre de tests possibles, choisir
  les tests désirés en modifiant classification.py (fin du fichier).}

\bigskip

Datasets:
\begin{verbatim}
donut
donut_simple
linear
tennis
glass
iris
optdigits_tes
optdigits_tra
krkopt
\end{verbatim}

\bigskip

Discretization:
\begin{verbatim}
--efd
--ewd
\end{verbatim}

Gain:
\begin{verbatim}
--gain
--gainratio
--gini
\end{verbatim}

\bigskip

Fichiers:

\begin{itemize}
\item{\verb-<Dataset>_data-: Dataset.}
\item{\verb-<Dataset>.py-: Extraction du dataset.}
\item{\verb-id3.py-: Implémentation de ID3.}
\item{\verb-kfcv.py-: K-Fold Cross-Validation.}
\item{\verb-classification.py-: Tests et génération des courbes.}
\item{\verb-discretization.py-: Méthodes de discrétisation.}
\end{itemize}

\bigskip

L'implémentation est basée sur les notes de cours.
Pour chaque noeud de l'arbre, on conserve les probabilités à
priori des éléments qui se retrouvent dans les fils, ce qui permet
d'assigner un label par défaut aux éléments pour lesquels on n'arrive
pas à descendre jusqu'à une feuille.

Lors de l'apprentissage, la profondeur de l'arbre n'augmente plus
lorsque l'entropie atteint 0 pour un noeud donné, ni lorsque tous les
éléments sont identiques (même si l'entropie n'est pas nulle).  Ces
deux limites sont adoptées par tous les algorithmes qu'on a pu voir
dans la littérature.

\subsubsection{PlayTennis}

\begin{figure}[ht]
  \begin{center}
  \includegraphics[width=120mm]{tennis_data_id3_graph}
  \caption{Arbre de décision pour le dataset PlayTennis (ID3).}
  \label{fig:tennis_data_id3_graph}
  \end{center}
\end{figure}

\autoref{fig:tennis_data_id3_graph} montre l'arbre obtenu pour le
dataset PlayTennis.  Le dataset étant simple, l'arbre de décision a
peu de noeuds.

\cleardoublepage

\subsubsection{Mushroom}

\begin{figure}[ht]
  \begin{center}
  \includegraphics[width=120mm]{mushroom_data_id3_graph}
  \caption{Arbre de décision pour le dataset Mushroom (ID3).}
  \label{fig:mushroom_data_id3_graph}
  \end{center}
\end{figure}

\autoref{fig:mushroom_data_id3_graph} montre l'arbre obtenu pour le
dataset Mushroom.  L'arbre induit reste simple (29 noeuds), alors que
le dataset a une taille importante (8000 points).  De plus, toutes les
feuilles ont une entropie de 0, ce qui suggère que l'arbre aura de
bonnes performances (par exemple sur une K-Fold cross validation).
\autoref{fig:mushroom_data_id3_500_kfcrossval} et
\autoref{fig:mushroom_data_id3_0_kfcrossval} illustrent ce dernier point:
l'arbre de décision obtenu pour id3 a des performances similaires à
KNN (en pratique, l'algorithme est également plus rapide).

\cleardoublepage

\subsection{Améliorations}

\subsubsection{Sélection de l'attribut à tester}

Les deux expressions alternatives du gain ont été implémentées
(GainRatio et impureté de Gini).
\autoref{fig:tennis_data_id3_gainratio_graph} et
\autoref{fig:tennis_data_id3_gini_graph} sont à comparer à
\autoref{fig:tennis_data_id3_graph} (dataset Tennis).
\autoref{fig:mushroom_data_id3_gainratio_graph} et
\autoref{fig:mushroom_data_id3_gini_graph} sont à comparer à
\autoref{fig:mushroom_data_id3_graph} (dataset Mushroom).  On observe
des arbres de tailles similaires pour les méthodes Gain et GainRatio,
alors que la méthode Gini produit des arbres avec un nombre de noeuds
plus important \autoref{tbl:id3_node_count}.
En outre, les taux de reconnaissance sont meilleurs
avec les méthodes Gain et GainRatio qu'avec Gini
(\autoref{fig:mushroom_data_id3_500_kfcrossval} sur une partie du dataset
Mushroom et \autoref{fig:mushroom_data_id3_0_kfcrossval} sur
l'ensemble du dataset).

\begin{figure}[ht]
  \begin{center}
  \includegraphics[width=120mm]{tennis_data_id3_gainratio_graph}
  \caption{Arbre de décision pour le dataset Tennis (ID3, GainRatio).}
  \label{fig:tennis_data_id3_gainratio_graph}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
  \includegraphics[width=120mm]{tennis_data_id3_gini_graph}
  \caption{Arbre de décision pour le dataset Tennis (ID3, Gini).}
  \label{fig:tennis_data_id3_gini_graph}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
  \includegraphics[width=120mm]{mushroom_data_id3_gainratio_graph}
  \caption{Arbre de décision pour le dataset Mushroom (ID3, GainRatio).}
  \label{fig:mushroom_data_id3_gainratio_graph}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
  \includegraphics[width=120mm]{mushroom_data_id3_gini_graph}
  \caption{Arbre de décision pour le dataset Mushroom (ID3, Gini).}
  \label{fig:mushroom_data_id3_gini_graph}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
  \includegraphics[width=120mm]{mushroom_data_id3_500_kfcrossval}
  \caption{Taux de reconnaissance sur K-Fold validation (ID3, Extrait
    du dataset Mushroom).}
  \label{fig:mushroom_data_id3_500_kfcrossval}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
  \includegraphics[width=120mm]{mushroom_data_id3_0_kfcrossval}
  \caption{Taux de reconnaissance sur K-Fold validation (ID3, dataset
    Mushroom).}
  \label{fig:mushroom_data_id3_0_kfcrossval}
  \end{center}
\end{figure}

\cleardoublepage

\subsubsection{Gestion des attributs continus}

Pour gérer les attributs continus, les méthodes EFD et EWD ont été
implémentées.  En pratique, elles ont des résultats similaires
\autoref{fig:glass_data_id3_discrete_0_kfcrossval}.  Les
tests ont étés effectués sur le dataset Donut (1000 points, 2
attributs, 2 classes, 10\% de bruit), ainsi que sur le
dataset Glass (214 points, 6 attributs, 7 classes)
\autoref{fig:glass_data_id3_efd_graph} illustre le dataset Glass à travers
l'arbre induit par ID3 sur le dataset discrétisé par EFD.

\autoref{fig:glass_data_id3_discrete_knn_0_kfcrossval} et
\autoref{fig:donut_data_id3_discrete_knn_0_kfcrossval} illustrent
l'utilité de la discrétisation sur des datasets à valeurs continues:
l'arbre induit par ID3 n'est efficace que lorsque les données sont
discrétisées et atteint la même précision que KNN.  On note également
que KNN est moins efficaces sur les données discrétisées que sur les
données continues, ce qui n'est pas surprenant.

\begin{figure}[ht]
  \begin{center}
  \includegraphics[width=120mm]{glass_data_id3_efd_graph}
  \caption{Arbre de décision pour le dataset Glass (ID3,
    discrétisation EFD).}
  \label{fig:glass_data_id3_efd_graph}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
  \includegraphics[width=120mm]{glass_data_id3_discrete_0_kfcrossval}
  \caption{Comparaison des méthodes de discrétidsation (K-Fold
    validation, ID3, dataset Glass + discrétisation).}
  \label{fig:glass_data_id3_discrete_0_kfcrossval}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
  \includegraphics[width=120mm]{glass_data_id3_discrete_knn_0_kfcrossval}
  \caption{Taux de reconnaissance sur K-Fold validation (ID3, dataset
    Glass + discrétisation).}
  \label{fig:glass_data_id3_discrete_knn_0_kfcrossval}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
  \includegraphics[width=120mm]{donut_data_id3_discrete_knn_0_kfcrossval}
  \caption{Taux de reconnaissance sur K-Fold validation (ID3, dataset
    Donut + discrétisation).}
  \label{fig:donut_data_id3_discrete_knn_0_kfcrossval}
  \end{center}
\end{figure}

\cleardoublepage

\subsubsection{\'Elagage}

La méthode d'élagage REP a été implémentée depuis le papier listé sur
le sujet et quelques recherches dans la littérature.  Lors de
l'apprentissage, 25\% des données sont mises à part pour l'élagage
(pruning set).  L'élagage se fait par itérations successives afin
d'obtenir l'arbre minimum qui a l'erreur minimale sur le pruning set.
\autoref{fig:mushroom_data_id3_gainratio_pruned_graph} illustre le
pruning sur un arbre de petite taille:  l'entropie en feuille devient
parfois non nulle (par rapport à l'arbre
\autoref{fig:mushroom_data_id3_gainratio_graph}) parce que le pruning
set ne représente pas toutes les données d'apprentissage.

\autoref{tbl:id3_node_count} donne la taille de l'arbre obtenu après
élagage pour différents datasets.  On observe un gain proportionnel
au rapport de la taille de l'arbre sur la taille du dataset.  Lorsque
l'arbre est très petit par rapport au dataset (e.g. Mushroom avec Gain
et GainRatio), l'effet de l'élagage est difficile à évaluer.

En pratique, le taux de reconnaissance d'un arbre est similaire qu'il
soit élagué ou non
(\autoref{fig:mushroom_data_id3_prune_500_kfcrossval} et
\autoref{fig:donut_data_id3_prune_discrete_500_kfcrossval}).

\begin{figure}[ht]
  \begin{center}
  \includegraphics[width=120mm]{mushroom_data_id3_gainratio_pruned_graph}
  \caption{Arbre de décision élagué pour le dataset Mushroom (ID3,
    GainRatio, REP).}
  \label{fig:mushroom_data_id3_gainratio_pruned_graph}
  \end{center}
\end{figure}

\begin{table}
  \begin{center}
  \begin{tabular}{ | c | c | c | c | c | c |}
    \hline
    Dataset & Taille & Gain & Complet & REP & Taux d'élagage \\ \hline \hline
    Mushroom & 8124 & Gain & 29 & 26 & 10\% \\ \hline
    Mushroom & 8124 & GainRatio & 26 & 23 & 12\% \\ \hline
    Mushroom & 8124 &Gini & 467 & 320 & 31\% \\ \hline
    Glass (EFD) & 214 & Gain & 38 & 15 & 61\% \\ \hline
    Glass (EFD) & 214 & GainRatio & 38 & 15 & 61\% \\ \hline
    Glass (EFD) & 214 & Gini & 38 & 17 & 55\% \\ \hline
    Donut (EFD) & 2000 & Gain & 145 & 130 & 10\% \\ \hline
    Donut (EFD) & 2000 & GainRatio & 145 & 130 & 10\% \\ \hline
    Donut (EFD) & 2000 &Gini & 145 & 130 & 10\% \\ \hline
  \end{tabular}
  \end{center}
  \caption{Taille de l'arbre induit selon plusieurs variantes de ID3
    (dataset Mushroom).}
  \label{tbl:id3_node_count}
\end{table}

\begin{figure}[ht]
  \begin{center}
  \includegraphics[width=120mm]{mushroom_data_id3_prune_500_kfcrossval}
  \caption{Effet de l'élagage sur le taux de reconnaissance
    (dataset Mushroom).}
  \label{fig:mushroom_data_id3_prune_500_kfcrossval}
  \end{center}
\end{figure}

\begin{figure}[ht]
  \begin{center}
  \includegraphics[width=120mm]{donut_data_id3_prune_discrete_500_kfcrossval}
  \caption{Effet de l'élagage sur le taux de reconnaissance
    (dataset Donut).}
  \label{fig:donut_data_id3_prune_discrete_500_kfcrossval}
  \end{center}
\end{figure}

\cleardoublepage

\section{SVM}

\subsection{Notes sur l'implémentation}

\bigskip

Test SVM:
\begin{verbatim}
make <Dataset>
python svm.py <Dataset>_data [<Kernel> <Kernel Parameter>] --test
\end{verbatim}

\bigskip

Classification:
\begin{verbatim}
make <Dataset>
python classification.py <Dataset>_data <Positive label> <Data Size>
\end{verbatim}
\emph{Note: \'Etant donné le grand nombre de tests possibles, choisir
  les tests désirés en modifiant classification.py (fin du fichier).}

\bigskip

Datasets:
\begin{verbatim}
donut
donut_simple
linear
linear_simple
tennis
glass
iris
optdigits_tes
optdigits_tra
krkopt
\end{verbatim}

\bigskip

Kernel \& Kernel parameter:
\begin{verbatim}
--linear_k
--poly_k <degré du polynome>
--rbf_k <gamma>
\end{verbatim}

\bigskip

Fichiers:
\begin{itemize}
\item{\verb-<Dataset>_data-: Dataset.}
\item{\verb-<Dataset>.py-: Extraction du dataset.}
\item{\verb-svm.py-: Implémentation du SVM.}
\item{\verb-kfcv.py-: K-Fold Cross-Validation.}
\item{\verb-classification.py-: Tests et génération des courbes.}
\item{\verb-discretization.py-: Méthodes de discrétisation.}
\end{itemize}

\bigskip

\subsection{2D tiny toy}
FIXME: Explications (dataset linear) + Images + kfcv

\subsection{Kernels}

FIXME: Explications + plein d'images de fonction de décision + kfcv

\subsection{Noise}

Le SVM implémenté ne gère pas le bruit.
FIXME: Afficher une fonction de décision, kernel RBF qui va bien.

\subsection{Multiclass Management}

Ce point n'a pas été implémenté.

\subsection{Unbalanced Dataset}

Ce point n'a pas été implémenté.

\section{Final benchmarking}

FIXME: Benchmark plusieurs classifieurs selon le dataset (kfcv).
Images + conclusion.

\end{document}
